# Monads? Arrows ? or Relative Monads ! A short hitchhiker's guide for handling quantum effects in a monadic way.

In this post, we will present you the way to structure quantum effects using arrows, and further, discuss why monads does not work in setting, and some insights of using relative monads.

## Superoperators as Arrows

### Introduction
Monands (in)famously play an important role in functional programming. Monads capture notions of computation. With the prevalence of quantum computing in today’s landscape, it is only sensible to ask whether monads can also describe quantum computations.

As part of the Adjoint School, we spent a lot of time in the last few months with [this paper by Juliana K. Vozotto, Thorsten Altenkirch, and Amr Sabry](https://arxiv.org/pdf/quant-ph/0501151) which details how one can use the concept of arrows which generalise the notion of a monad, to model quantum computations. One question we encountered a lot while working with this paper is why exactly monads aren’t strong enough to model quantum computation. Heuristically, this seems to be true, but this question deserves a concrete mathematical answer. In the following, we will highlight key concepts and constructions from the paper.

### Basics of Quantum Mechanics
In quantum mechanics, the state of a system is usually described as a normalised vector in some Hilbert space. Since we are interested in quantum computing, which only utilises a finite number of qubits, we thankfully only need to worry about finite-dimensional Hilbert spaces. A general quantum state then has the following form:
$$
\lambda_1|x_1\rangle+\ldots+\lambda_n |x_n\rangle
$$
where we borrow some of the physicists' bra-ket-notation to denote basis elements and the $\lambda_i$s are complex numbers satisfying $\sum_{i=1}^n|\lambda_i|^2=1$. Such a vector describes the state of a system in the following way:
- The $x_i$s are the so-called eigenstates of our system, that is, the classical states we find our system in if we are to measure it. For example, if our system is the spin of a particle, we have the two eigenstates `up` and `down`, if our system is electron of a hydrogen atom, then our eigenstates are the energy levels at which the electron can exist.
- The $\lambda_i$s describe the probability with which one can expect to measure a given eigenstate, namely, we measure the eigenstate $x_i$ with a probability of $|\lambda_i|^2$.


If we are given a finite set $X$ of eigenstates, the states of our system then exist in the free complex vector space $FX$ generated by the set $X$. For example, if we are given the set $B=\{0,1\}$, we get the vector space $\mathbb{C}^2$ with basis vecors $|0\rangle$ and $|1\rangle$. This being the qubit.

An operation on a quantum system is given by a unitary matrix acting on the underlying vector space. Unitarity just makes sure that normalised vectors get mapped to normalised vectors. We can model the space of linear maps between two systems with eigenstates $X$ and $Y$ as $\text{Lin}(X,Y)=\text{Map}(X,FY)$.

### Density Matrices and Superoperators
If we are to make a measurement as part of a quantum algorithm, our nicely defined quantum state collapses into a probabilistic mess of eigenstates. If we measure the state $\frac{1}{\sqrt{2}}\left(|0\rangle+|1\rangle\right)$, we get $0$ and $1$ each with a probability of 50%. You might think we can simply repackage this stochastic information back as a superposition of eigenstates, but we encounter the problem that non-equivalent states can have the same probabilities. If we measure the state $\frac{1}{\sqrt{2}}\left(|0\rangle-|1\rangle\right)$, we also get both $0$ and $1$ with a probability of 50%. We want to find a way to talk about these more generalised stochastical states.

This is where density matrices come into play. Given a set of eigenstates $X$, the space of density matrices $\text{Dens}(X)$ describing the state of our system is given by $F(X\times X)$, the free vector space generated by pairs of eigenstates, or isomorphically $FX\otimes FX$. We have a way of embedding the space of the earlier discussed pure states $FX$ into $\text{Dens}(X)$ given by
$$
\begin{aligned}
\text{pureD}: & FX\to \text{Dens}(X) \\
& \sum_{x\in X}\lambda_x |x\rangle \mapsto \sum_{x\in X}\sum_{y\in X} \lambda_x \overline{\lambda_y} |x\rangle\otimes |y\rangle
\end{aligned}
$$
As the name suggests, we can notate density matrices as matrices.
$$
\begin{aligned}
& \text{pureD} \, |0\rangle =
\begin{pmatrix}
1 & 0 \\
0 & 0 \\
\end{pmatrix} \\
& \text{pureD} \, |1\rangle =
\begin{pmatrix}
0 & 0 \\
0 & 1 \\
\end{pmatrix} \\
& \text{pureD} \left(\frac{1}{\sqrt{2}}\left(|0\rangle  -|1\rangle\right)\right) = \frac{1}{2}
\begin{pmatrix}
1 & -1 \\
-1 & 1 \\
\end{pmatrix}
\end{aligned}
$$
Density matrices can also represent some non-pure states that only describe a system probibalistically. The matrix
$$
\begin{aligned}
\begin{pmatrix}
\frac{1}{2} & 0 \\
0 & \frac{1}{2}
\end{pmatrix} \in \text{Dens}(B)
\end{aligned}
$$
is not in the image of $\text{pureD}$ but it nevertheless tells us that our system will be in the state $0$ with a probability of 50% and in the state $1$ with a probability of 50% when measured.

Superoperators are linear maps between density matrices. We model them as $\text{Super}(X,Y)=\text{Map}({X\times X},{\text{Dens}(Y)})$. We can embed linear maps into superoperators in the following way.
$$
\begin{aligned}
\text{lin2super}: & \text{Lin}(X,Y) \to \text{Super}(X,Y) \\
& \text{lin2super} \, f \, (x_1, x_2) = f(x_1)\otimes\overline{f(x_2)}
\end{aligned}
$$
where $\overline{f(x)}$ is defined as taking the complex conjugate of $f(x)$ component-wise.

We now know that we can embed our previously discussed vector spaces and unitary maps into the framework of density matrices and superoperators.

### Superoperators as Arrows
While we do believe, that superoperators and therefore measurement can't be implemented using a monad. They can be implemented as what are known as arrows which generalise the notion of a monad. Arrows consist of a type constructor $\text{Ar}$ which takes two inputs $X$ and $Y$ and returns $\text{Ar}(X,Y)$, together with functions

$$
\begin{aligned}
\text{arr} & : \text{Map}(X,Y) \to \text{Ar}(X,Y) \\
{>\!\!>\!\!>} & : \text{Ar}(X,Y) \times \text{Ar}(Y,Z) \to \text{Ar}(X,Z) \\
\text{first} & : \text{Ar}(X,Y) \to \text{Ar}(X\times Z,Y \times Z)
\end{aligned}
$$

which need to satisfy a list of properties. We can now implement superoperators as arrows in the following way.
$$
\begin{aligned}
\text{arr} & : \text{Map}(X,Y) \to \text{Super}(X,Y) \\
\text{arr} & \, f (x_1,x_2) = |f(x_1)\rangle \otimes |f(x_2)\rangle \\
{>\!\!>\!\!>} & : \text{Super}(X,Y) \times \text{Super}(Y,Z) \to \text{Super}(X,Z) \\
& \, (f{>\!\!>\!\!>} g) (x_1,x_2) = f(x_1,x_2) {>\!\!>\!\!=} g \\
\text{first} & : \text{Super}(X,Y) \to \text{Super}(X\times Z,Y \times Z) \\
\text{first} & \, f ((x_1,z_1),(x_2,z_2)) = \pi_{23} \left(f(x_1,x_2)\otimes |z_1\rangle \otimes |z_2\rangle\right)
\end{aligned}
$$
where ${>\!\!>\!\!=}:FX\times \text{Map}(X,FY)\to FY$ is the monadic bind which, given an input and a function, applies that function to the input and $\pi_{23}:FX\otimes FX \otimes FZ \otimes FZ \to FX\otimes FZ\otimes FX\otimes FZ$ just swaps the second and third component in the tensor product.

We now see that the notion of arrows plays nicely with superoperators. Using $\text{arr}$ we can transform simple maps into superoperators. ${>\!\!>\!\!>}$, which generalises the monadic bind operation ${>\!\!>\!\!=}$, lets us compose superoperators. And $\text{first}$ lets us apply superoperators only on specified parts of a density matrix.

## Monads vs. Arrows vs. Relative Monads

### Why we could NOT use monads ? 

A short reminder, in functional programming, for a monad $m$, the
multiplication rule corresponds directly to **join :**
$m (m a) \to m a$. In reality, it is represented by a mutation **bind
:** $ma \to (a \to mb) \to mb$, where $a \to mb$ is a kleisli arrow.\
**-- Why do we believe monads fail in this setting ?**

One of the main problems within this setting is the typing.

It is shown above that monads work perfectly for the computations within
vector spaces, as simply the linear map. However, when it comes to
measurements and superoperators, the density map $D$ is desired, it is
represented through the bra-ket operation as $D := (a, a)$, which $a$ is
the original basis type. Categorically, of the form $(A, A^{op})$.

Hence unlike linear maps of the type: $$a \to m b$$

Superoperators are of the type: $$(a,a) \to m b$$ They do NOT form
Kliesli arrows: given two kleisli arrows $\textcolor{red}{f: A \to MB}$
, $\textcolor{teal}{g: B \to MC}$, to form the kleisli category, we need
the codomain of $\textcolor{red}f$ to match the domain of
$\textcolor{teal}g$. Not only it is lack of well defined identity, the
composition also doesn't work out: Suppose we have
$\textcolor{red}{f: A\times A \to MB}$ and
$\textcolor{teal}{g: B\times B \to MC}$, $\textcolor{teal}g$ is
expecting $B\times B$ as input.

To attack this problem, there are several ways we could try out:

-   When M is a monoidal monad, and it interacts nicely with the
    monoidal product. which requires too much additional structures.

-   Using Comonoids:

    Every object $A$ is equipped with a comonoid structure (i.e., a
    diagonal map $A \to A\times A$ and a counit $A\to 1$. (As a
    Copy-Discard Category)

-   Use some other structures such as arrows.

The paper gives a solution using arrows, observing that the arrows,
which intake inputs of the form of a pair $(A, A^{op})$, are also
generalization of strong monads (*Caution : it does not capture
non-strong monads*). It solves the typing problem elegantly and also
naturally extends what strong monads could do simply by using the
kleisli arrows.

A more general way of seeing this mismatch is simply that, measurements
do not live in vector spaces, which monads could model. Instead, they
live in (for finite dimensional quantum theory), finite dimensional
vector spaces. And finite (or even infinite) dimensional vector spaces,
instead of being an endofuctor, is a (forgetful) functor :
$$U : \textbf{FinVect} \to  \textbf{Set}$$

It is not an endofuctor hence could not directly form a monad.

However, this statement received some doubts from the community,
especially, Vladimir Zamdzhiev questioned that:

**-- Is it really true that one could say monads do not work for
functional quantum computing ?**

Because there have been several breakthroughs since, that people found
adjunction situation between those functors, which could still lead us
to monads.

Especially, Peter Selinger and Benoit Valiron's **Quantum Lambda
Calculus** (Benoit's PhD thesis) potentially gave the model including
the (co)monad along with the calculus, however they did not find it in
quantum setting back then. Further Kenta Cho and Abraham Westerbaan's
**Von Neumann Algebras form a Model for the Quantum Lambda Calculus**,
found a model of the calculus based on von Neumann algebras, shows that
there is indeed an adjunction between $\textbf{vNA}_{MIU}^{OP}$ and
**Set**. More explicitly, between the forgetful functor:
$$F : \textbf{vNA}_{MIU}^{OP} \xrightarrow {nsp}  \textbf{Set}$$ and
free vector space functor:
$$U : \textbf{Set} \xrightarrow{l^{\infty}} \textbf{vNA}_{MIU}^{OP}$$
$F$ is left adjoint to $U$, hence we could construct a monad
$$T=l^{\infty}\circ nsp$$ from this adjunction.

Some remarks about this state monad is, that it controls the process,
instead of being a computational monad. And some interesting comments
from Peter Selinger via Benoit Valiron : *My supervisor used to say that
using vNA is cheating: it is big enough to contain "everything and the
kitchen sink".*

But as one can easily observe, if we restrict into finite dimension, $U$
could not be right adjoint to $F$, because $U$ has finite dimensions,
could not land in $F$ which has infinite dimensions.

**-- Conclusion: Monads do not exist in finite dimension setting, but do
exist in infinite dimension setting, arise from the adjunction.**

Although real life quantum computing are normally finite dimensional,
when quantum measurements yield discrete outcomes, while infinite
dimensions would also imply continuous observables (e.g.,
position/momentum), which are harder to control.

###  One step further from arrows : Relative Monads 

If we view the problem of the failure of using monads as the
incapability of constructing a monad from a functor mapping two
different categories which, one lives in finite dimension and other
infinite dimension, then as the title of the paper says, **Monads need
not be endofunctors!** The **Relative Monad** it defines on a functor
$J \in \mathbf{J} \to \mathbf{C}$ between two different categories
yields another solution to the problem which has more monadic tastes. It
is even a generalization of arrows: technically, an arrow on
$\mathbf{J}$ is the same thing as a relative monad on the Yoneda
embedding $Y\in \mathbf{J} \to [\mathbf{J}^{op}, \textbf{Set}]$.

Moreover, in infinite dimension setting, compared to the monad arises
from the adjunction, we believe relative monads also exist and it
potentially provides alternatively a more trivial and
easier-to-manipulate structure, which is more natural in functional
programming to handle computational effects. (And potentially, to
capture something arrows could not.)
